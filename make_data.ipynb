{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MeCab\n",
    "import mojimoji\n",
    "import gensim\n",
    "m = MeCab.Tagger('-Ochasen')\n",
    "m.parse('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 製品名を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brackets(text):\n",
    "    if text.find('(') != -1:\n",
    "        return text[:text.find('('):]\n",
    "    return text\n",
    "\n",
    "def replace_summary(row):\n",
    "    item = extract_brackets(mojimoji.zen_to_han(row['item'], kana=False))\n",
    "    return row['accident_summary'].replace(item, ' ').replace('使用中', ' ').replace('使用', ' ')\n",
    "\n",
    "df['replaced_as'] = df.replace('(【|【)', '(', regex=True).replace('(】|】)', ')', regex=True).apply(lambda row: replace_summary(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# パースするところ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopword():\n",
    "    with open('./wmd/stopwords.txt', 'r') as f:\n",
    "        stop_words = f.read().splitlines()\n",
    "    return stop_words\n",
    "\n",
    "stop_words = read_stopword()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# word2vecのベクトルに使う単語を抽出するところ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data//scdv-ver_input_injured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後の名詞を取る\n",
    "def extract_noun(sentence):\n",
    "    for chunk in m.parse(sentence.split('。')[0]).splitlines()[-2::-1]:\n",
    "        feature = chunk.split('\\t')\n",
    "        if '名詞' in feature[3]:\n",
    "            return feature[2]\n",
    "    return ''\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    nouns = ''\n",
    "    for chunk in m.parse(sentence.split('。')[0]).splitlines()[:-1]:\n",
    "        feature = chunk.split('\\t')\n",
    "        if '名詞-サ変接続' in feature[3] or '負う' in feature[2]:\n",
    "            nouns += feature[2] + ','\n",
    "    return nouns[:-1]\n",
    "\n",
    "def delte_double_words(row):\n",
    "    summary = ''\n",
    "    for r in list(set(row.split(','))):\n",
    "        summary += r + ','\n",
    "    return summary[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['replaced_as'] = df['replaced_as'].replace('[0-9]+', '0', regex=True).replace('(０|１|２|３|４|５|６|７|８|９)+', '0', regex=True)\n",
    "noun = df['replaced_as'].apply(lambda row: extract_noun(mojimoji.zen_to_han(row, kana=False)))\n",
    "df['parsed_summary'] = df['replaced_as'].apply(lambda row: parse_sentence(mojimoji.zen_to_han(row, kana=False), stop_words, other_flag)) + noun\n",
    "df['parsed_summary'] = df['parsed_summary'].apply(lambda row: delte_double_words(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/word2vec-ver_input_injured.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 怪我を負ったレポートに対してword2vecで平均ベクトルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/word2vec-ver_input_injured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../entity_vector/entity_vector.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_vector(words, model, vec_size):\n",
    "    avg_vec = np.zeros(vec_size, )\n",
    "    for word in words:\n",
    "        try:\n",
    "            avg_vec += model[word]\n",
    "        except:\n",
    "            avg_vec += np.zeros(vec_size, )\n",
    "    return avg_vec / len(words)\n",
    "\n",
    "vec_size = model.vector_size\n",
    "avg_vec = df['parsed_summary'].apply(lambda row: calc_avg_vector(row.split(','), model, vec_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat dataframe\n",
    "col_list = ['d'+str(i) for i in range(avg_vec[:1][0].shape[0])]\n",
    "out_df = pd.DataFrame(columns=col_list)\n",
    "for sv in avg_vec:\n",
    "    out_df = out_df.append(pd.Series(sv, index=out_df.columns), ignore_index=True)\n",
    "\n",
    "out_df.to_csv('../output/word2vec_out.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
